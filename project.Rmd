---
title: "Machine Learning"
output:
  html_document: default
  pandoc_args:
  - +RTS
  - -K512m
  - -RTS
  pdf_document: default
  word_document: default
geometry: margin=2cm
---

### Executive Summary

This report applies machine learning techniques to data sets produced as part of a Human Activity Recognition project.  The original research effort involved 6 young male participants (aged 20-28) performing weight lifting correctly (class A) as well as incorrectly in defined manner representative of 4 common mistakes (class B-E).  

Thru the machine learning algorithms in this report, we will attempt to predict the measured outcomes of participant performance (class A-E represented by variable classe) using other variables from the dataset.  The original study can be found at http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises.  

### Data Analysis
```{r loadlibrary, echo=FALSE, message=FALSE, warning=FALSE}
library(caret)
library(rpart)
library(randomForest)
library(gbm)
library(doParallel)
registerDoParallel(cores=3)
```

Two datasets are referenced for this machine learning effort.  The first, https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv, will form the basis of all training and testing for the developed algorithms.  The second, https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv, will be the basis for scoring once algorithms have been developed. 

```{r readinitial}
xtest <- read.csv("pml-testing.csv")
xtrain <- read.csv("pml-training.csv")
str(xtrain[1:20])
```

Data dimensions for the training dataset show `r nrow(xtrain)` observations and `r ncol(xtrain)` variables.  Evaluation of the training data through the str() function in R reveals numerous NA, blank, and non-numeric values including DIV/0!.  These values result in column representation as factor rather than numeric.  

The data is read again handling these values as NA.

```{r readdata}
xtest <- read.csv("pml-testing.csv",na.strings=c("NA","","#DIV/0!"))
xtrain <- read.csv("pml-training.csv",na.strings=c("NA","","#DIV/0!"))
```

```{r dispdata}
str(xtrain[1:20])
```

With the columns properly determined as numeric, focus turns to the large number of NA values within many columns.  These values need to be addressed prior to building models.  Columns with greater than 80% of the data NA are eliminated as there is insufficient information within these columns to accurately provide NA replacement values.  Additionally, nearZeroVar function is used to identify and eliminate columns containing few unique values as these will provide minimal to no value for future predictions.  Finally, the values in the first 7 columns are descriptive in nature to identify the original subject or timeframe in which observations were taken.  These provide no value in interpreting data for new subjects and are eliminated.

```{r datascrub}
lst <- colnames(xtrain)
count.na <- sapply(lst,FUN=function(x,xtrain){sum(is.na(xtrain[,x]))},xtrain)
xtrain <- xtrain[,-which(count.na>=.8*nrow(xtrain))]
nzv <- nearZeroVar(xtrain,saveMetrics=TRUE)
xtrain <- xtrain[,nzv$nzv==FALSE]
xtrain <- xtrain[,-c(1:7)]
```

Dataset after data cleaning now reflects `r nrow(xtrain)` observations and `r ncol(xtrain)` variables.  Looking at the outcome variable, classe, the observed values align to original expectations for this column `r unique(xtrain$classe)`.  

```{r datafinal}
str(xtrain[1:20])
```

###Model Validation Approach

Prior to initial model build, data is partitioned into a training set for initial machine learning steps, a validation/tuning set which will be leveraged after initial model build for fine tuning, and a test set to be evaluated one time by final model.  The luxury to divide into 3 sets exists due to the large number of observations in the original research effort.  

Once a final model is identified, tuning set will be used to estimate the Out of Sample error, and the testing set outcome will be compared to this estimate.

A seed is set to ensure reproducibility of results in future.

```{r datapartition}
set.seed(32323)
inTrain <- createDataPartition(y=xtrain$classe,p=.6,list=FALSE)
training <- xtrain[inTrain,]
testing <- xtrain[-inTrain,]
inVal <- createDataPartition(y=testing$classe,p=.6,list=FALSE)
tuning <- testing[inVal,]
testing <- testing[-inVal,]
```

The training set now contains `r nrow(training)` observations and `r ncol(training)` variables, the validation/tuning set `r nrow(tuning)` observations and `r ncol(training)` variables, and the testing set `r nrow(testing)` observations and `r ncol(testing)` variables.    

Within the validation and tuning sets, k fold cross validation with k=10 will be leveraged.  

```{r crossval}
fitControl <- trainControl(method = "cv",number = 10)
```

###Initial Model

As the outcome to be predicted is a classification, various non-linear classification models will be evaluated first.  Four are selected for initial pass including the Classification and Regression Tree (CART), k Nearest Neighors (knn), Random Forest, Gradient Boosted Machines (GBM)  

```{r initialmod, message=FALSE, warning=FALSE}
modelFitpart <- train(classe~.,data=training,method="rpart",trControl=fitControl)

modelFitknn <- train(classe~.,data=training,method="knn",trControl=fitControl)

modelFitRF <- train(classe~.,data=training,method="rf",trControl=fitControl, allowParallel=TRUE)

modelFitgbm <- train(classe~.,data=training,method="gbm",trControl=fitControl,verbose=FALSE)
```

Comparison of the initial models is completed by looking at the difference in acccuracy for each.  In the following plot with confidence level .992, we can reject the null hypothesis that there is no difference in model accuracy for all pairings of models.  We can also see that CART provides the least accuracy, knn performs substantially better than CART but less than Random Forest or GBM.  We see that Random Forest performs better than GBM with very tight confidence interval.  Random Forest is the best option among these initially evaluated models.

```{r evalmod}
resamps <- resamples(list(KNN=modelFitknn,CART=modelFitpart,RF=modelFitRF,GBM=modelFitgbm))
difValues <- diff(resamps)
dotplot(difValues, metric="Accuracy")
```

The confusion matrix for Random Forest reflects low error rates for each predicted value.  In order to achieve this level of accuracy, a total of `r nrow(varImp(modelFitRF)$importance)` of the original `r ncol(training)` variables were leveraged.  As accuracy will always improve with larger number of factors in modeling, it would be valuable to know if comparable accuracy could be obtained with smaller set of variables.

```{r modconfusion}
modelFitRF$finalModel$confusion
```

###Model Tuning

In an attempt to reduce the number of factors while achieving comparable accuracy, three tuning approaches to the selected Random Forest algorithm are tried.  

The first takes the variables of importance that contributed to 80% of the model fit related to each of the previous models, excluding knn as the varImp() function does not work with this model.  

```{r ImpVarmod, message=FALSE, warning=FALSE}
varImppart <- as.matrix(varImp(modelFitpart)$importance)
varImpRF <- as.matrix(varImp(modelFitRF)$importance)
varImpgbm <- as.matrix(varImp(modelFitgbm)$importance)
keepPart <- row.names(varImppart)[which(varImppart>20)]
keepRF <- row.names(varImppart)[which(varImpRF>20)]
keepgbm <- row.names(varImppart)[which(varImpgbm>20)]
keepcol <- unique(c(keepPart,keepRF,keepgbm))
keepidx <- colnames(xtrain)%in%keepcol
keepidx[52] <- TRUE
trainImp <- training[,keepidx]
tuneImp <- tuning[,keepidx]

modelFitRFimp <- train(classe~.,data=tuneImp,method="rf",trControl=fitControl, allowParallel=TRUE)
```

The resulting confusion matrix reflects low error rates for each predicted value using a total of `r nrow(varImp(modelFitRFimp)$importance)` variables.

```{r modconfusionimp}
modelFitRFimp$finalModel$confusion
```

The second attempt uses Principal Components Analysis in conjunction with Random Forest.

```{r PCAmod}
preProc <- preProcess(training[,-52],method="pca")
trainPC <- predict(preProc,training[,-52])
tunePC <- predict(preProc,tuning[,-52])

modelFitRFpca <- train(tuning$classe~.,data=tunePC,method="rf",trControl=fitControl,allowParallel=TRUE)
```

The resulting confusion matrix reflects low error rates for each predicted value using a total of `r nrow(varImp(modelFitRFpca)$importance)` variables..

```{r modconfusionpca}
modelFitRFpca$finalModel$confusion
```

The final tuning approach considers the actual variables.  An ideal goal would be to have a high performing model with minimal number of factors while providing clear context for purpose of each selected variable.  The variables selected by each for the two previous tuning Random Forest methods include:

``` {r RFimp}
varImp(modelFitRFimp)
```

``` {r pcaimp}
varImp(modelFitRFpca)
```

The first tuning method selected variables from the x,y and z dimensions for many of the factors, but not from all 3 dimensions for any of the factors.  It would be difficult to explain why a subject movement in 2 of 3 dimensions was relevant, but not the third.  The Principal Components Analysis variables are actually each a combination of many factors, and as a result can be difficult to interpret.  Other factors are available in the original dataset that seem to summarize the original x,y,z measurements without relying on the dimensional elements.  The third approach will leverage Random Forest reducing factors to only those with no x,y and z dimension variable names.

```{r NVmod}
xstr <- grepl("_x|_y|_z",colnames(training))
nvtrain <- training[,!xstr]
nvtune <- tuning[,!xstr]

modelFitRFnv <- train(classe~.,data=nvtune,method="rf",trControl=fitControl, allowParallel=TRUE)
```

The resulting model confusion matrix reflects low error rates for each predicted value using a total of `r nrow(varImp(modelFitRFnv)$importance)` variables.  

```{r modconfusionnv}
modelFitRFnv$finalModel$confusion
```

``` {r nvimp}
varImp(modelFitRFnv)
```

Comparison of the 3 tuned Random Forest models with the original Random Forest model is now completed looking at the difference in acccuracy.  In the following plot with confidence level .992, we cannot reject the null hypothesis that there is no difference in model accuracy for the RF variables of importance to RF non-vector.  Random Forest alone performs substantially better than RF with PCA, but only slightly better than RF with non-vector and RF with variables of importance.  Given the benefit of fewer factors required, and the ease of explanation of what each of those factors are, the RF with non-vector variable algorithm is selected.

```{r modComp}
resamps <- resamples(list(RF=modelFitRF,RFpca=modelFitRFpca,RFnv=modelFitRFnv,RFimp=modelFitRFimp))
difValues <- diff(resamps)
dotplot(difValues, metric="Accuracy")
```

###Out of Sample Error

In order to estimate the Out of Sample error, a missclassification function is created.  That function is applied in turn to the tuning data to estimate the out of sample error.  Comparison is then completed using the testing data originally set aside.  We can see that the estimated out of sample error is very close to actual performance against the testing data.

```{r outsamperr}
missClass <- function(values,prediction){sum(prediction!=values)/length(values)}
missClass(nvtune$classe,modelFitRFnv$finalModel$predicted)
missClass(testing$classe,predict(modelFitRFnv,testing))
```

A comparable assessment can be completed for the original Random Forest algorithm to reflect the Out of Sample estimated error againt training set, and the resulting error when applied to testing data.  We can see that accuracy would in fact be somewhat higher with this algorithm than selected algorithm, but as stated earlier, there is also value in providing clear context behind the model factors selected.

```{r outsamperrRF}
missClass(training$classe,modelFitRF$finalModel$predicted)
missClass(testing$classe,predict(modelFitRF,testing))
```

###Project Submission

Finally, the selected algorithm is run against the project provided test data set for submission.

```{r writeans}
testPred <- predict(modelFitRFnv, xtest)
testPred
answers <- as.character(testPred)
setwd("Answers")
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
setwd("..")
```
